<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<title>
  
    Archive: 2019
  
</title>

<meta property="og:type" content="website">
<meta property="og:title" content="Joy&#39;s blog">
<meta property="og:url" content="http://yoursite.com/archives/2019/index.html">
<meta property="og:site_name" content="Joy&#39;s blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Joy&#39;s blog">


  <link rel="alternative" href="/atom.xml" title="Joy&#39;s blog" type="application/atom+xml">



  <link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="/perfect-scrollbar/css/perfect-scrollbar.min.css">
<link rel="stylesheet" href="/styles/main.css">






</head>
<body class="monochrome">
  <div class="mobile-header">
  <button class="sidebar-toggle" type="button">
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
  </button>
  <a class="title" href="/">Joy&#39;s blog</a>
</div>

  <div class="main-container">
    <div class="sidebar">
  <div class="header">
    <h1 class="title"><a href="/">Joy&#39;s blog</a></h1>
    
    <div class="info">
      <div class="content">
        
        
          <div class="author">Joy</div>
        
      </div>
      
        <div class="avatar">
          
            <a href="/about"><img src="/images/cc/1.jpg"></a>
          
        </div>
      
    </div>
  </div>
  <div class="body">
    
      
        <ul class="nav">
          
            
              <li class="archive-list-container">
                <a href="javascript:;">Archive</a>
                <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">2</span></li></ul>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="/" title="Homepage">Homepage</a>
              </li>
            
          
            
              <li>
                <a href="/archives" title="By Year">By Year</a>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="https://github.com/denjones" title="Github" target="_blank" rel="noopener">Github</a>
              </li>
            
          
        </ul>
      
    
  </div>
</div>

    <div class="main-content">
      
        <div style="max-width: 1000px">
      
          

  
  
    
      
      
      <section class="archives-wrap">
        <div class="archive-year-wrap">
          <h1><a href="/archives/2019" class="archive-year">2019</a></h1>
        </div>
        <div class="post-list">
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/09/24/Project-Summary/">
  Project Summary
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/09/24/Project-Summary/"><span class="article-date">
  2019-09-24
</span>
</a>
        

        

      </div>
      <div class="article-entry">
        
          <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


<p>The applications of machine learning expand beyond our general assumptions. From database mining, web search engine, to spam detector, the ever-improving technology of machine learning benefits companies and organizations across the globe. One may consider this technology to be hard to grasp and complex, yet it is straightforward and can also be simply explained with linear regression models. Using python, we want to test machine learning’s ability to make accurate predictions on the world ranking of 196 players on the professional golf tour using performance-based data. </p>
<p>For this project, we are using the 70 data ranging from players’ ages, driving distances, fairway hits, total stroke gained, to green in regulations from the year of 2017. The preliminary process of this project is constructing histograms for each row of data to obtain general information such as patterns and outliers using the plotting library Matplotlib in python.  </p>
<center>June 23rd,2019 </center>

<p>Today we talked about the definition of machine learning by Tom Mitchell. Using linear regression as a suitable example to explain the definition, I learned how process of machine learning is simplified to “T”, “E” and “P”. “T” as in tasks, “E” as in experiences, and “P” as in Performance. In linear regression, the line of best fit is determined by the data or inputs on the graph, and an equation is used in this process of determination.  To explain linear regression using the definition of machine learning, the task of the model is to provide an estimate value, and the model’s performance improve as it experiences more data.</p>
<p>In machine learning, terms like training dataset and learning dataset are used. Training dataset are used to train a prediction model and the model is then applied to the test dataset. One error involved in the making of prediction models is overfitting. Overfitting describes a situation where the model fits the training dataset too well but applies poorly in the test dataset. The model may be too complex for the dataset given or the training dataset is too limited to be applied to the test dataset. </p>
<p>One of the ways to measure the performance of a regression model is by mean squared error(MSE). The equation of </p>
<p><img src="/2019/09/24/Project-Summary/1.png" alt="3.jpg"></p>
<p>is used, where equals the number of observation, y’ is the actual value and y is the prediction. The value of the equation 0 equals a perfect model, and the value increases aa the prediction diverges. However the value of MSE can often be too large to measure the performance clearly and the square root of MSE is used (RMSE) Is used. Another way of evaluating linear regression is using R squared(Coefficient of determination). The equation of</p>
<p><img src="/2019/09/24/Project-Summary/2.png" alt="3.jpg"></p>
<p>and</p>
<p><img src="/2019/09/24/Project-Summary/3.png" alt="3.jpg"></p>
<p>is used for this method of evaluation and the value is given between 1 to 0. And when the value equals to 1, the prediction model is perfect. </p>
<center>June 29th, 2019</center>

<p>Today, we discussed data visualization. The uses of bar graph and for-loops are effective ways to quickly visualize a large quantity of data to find the outliers or patterns. There are several ways to organize and clean the raw data into easily understandable format. Examples of that is finding the missing data using .isnull() and converting the data with thousandth separator into integers using the thousands=’,’. Another way to better understand the data is through the use of encoding. One example of that is the one-hot encoding that converts categorical values into integer values that consists of 0s and 1s. The use of encoding allows us to effectively compare data that consist of categorical information. </p>
<p>Pearson correlation is also another topic we discussed. When comparing two numerical data, a Pearson correlation coefficient is a number between 1 and -1 that how the two variables are linearly related. For categorical and numerical data, mean and medians are used to measure their correlation. For two categorical data however, information gain is used. The term entropy was also introduced. The definition of entropy in information is the measure of randomness, disorder or uncertainty in data. The equation for entropy is: </p>
<p><img src="/2019/09/24/Project-Summary/4.png" alt="3.jpg"></p>
<p>Entropy in a way defines how a decision tree splits its data, and we can also utilize python to  calculate entropy. The decision tree is a tree like structure that maps out the model of decisions and their possible consequences. The information gain is also used in the construction of the decision tree, and the algorism will always maximize the information gain. The equation for information gain is:</p>
<p><img src="/2019/09/24/Project-Summary/5_1.png" alt="3.jpg"></p>
<center>July 6th, 2019</center>

<p>Today, we discussed the different types of machine learning problems. The problems have two major types: supervised and unsupervised. In supervised ML problems, all data are labelled and the algorithms learn to predict the output from the input. On the other hand, unsupervised problems have only input but no label and no corresponding output (no right answers). Under the branch of unsupervised problems, cluster is a type that allows you to get insights of the data through organizing and splitting. Under the supervised branch are classification and regression. Classification involves categorical data whereas regression involves numerical data. Under the regression type are multivariable and single variable. Then we further discussed about the decision tree. The training process builds the tree using the information gain from top to bottom. The decision tree for classification has root nodes as a features and branch as numerical values (mean). For regression, however, the outputs of the tree are numbers. The building of the decision tree is a recursive process. </p>
<p>Another topic related to the decision tree is the decision boundary that can be used for separating a set of data. The decision boundary is the line that separates, and it can be linear or nonlinear. Every decision tree has its own decision boundary. The decision boundary for regression is shown through isophote. And the decision boundary for classification is determined by the numbers on the nodes. </p>
<p><img src="/2019/09/24/Project-Summary/3.jpg" alt="3.jpg"></p>
<center>July 13th, 2019</center>
Today, we discussed linear regression, neutral network, and k-means clustering. Linear regression models relationship between two quantitative variables. The variable x is the predictor and the variable y is the outcome or the dependent variable. For more than one predictors, we are dealing with vector. We use the loss function as a criteria to measure how good a prediction model does in linear regression. We want to minimize the mean squared error in our outcome variable and the predicted outcomes; in order to do that, we take the closed form(which is taking the derivative and set it equal to zero) of the loss function and define the best-fitting line. 

<p><img src="/2019/09/24/Project-Summary/4.jpg" alt="4.jpg"></p>
<p>We also mentioned cross validation: a way to best utilize training data and testing data for more accurate predictions. The method splits data into 5 sections. First, section 1 is used as testing data and section 2, 3, 4 and 5 are used as training data. Then, section 2 is used as testing data and the rest is used a training data. This process is repeated to all five sections for 5 tests. Finally, the average of 5 test results is taken. </p>
<p><img src="/2019/09/24/Project-Summary/5.png" alt="5.jpg"></p>
<p>Similar to linear regression, the neural network model is a set of algorithms. But unlike linear regression, which has limited expression and weak representation, neural network is multilayered and can be as complex as having millions of parameters. While the linear regression models has parameters, neural network has parameters as well as hidden layers of neutrons. To find all parameters in a neural network, the activation function is very important in introducing non-linear properties into the network. A neural network without the activation function is simply a linear regression model. Common activation functions are sigmoid and rectified linear unit. </p>
<p>Finally, we discussed k-means clustering, a method of identifying the number of centroids and assign each data point to the closest centroid as a cluster, while keeping the number of the centroids as low as possible. The algorithm first initialize the centroids through random selection and assign data points to the centroids. Then through an iterative process of checking data point assignment changes and updating centroids, the algorithms ends when there are no more changes made. </p>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/09/24/Coding-Demonstration/">
  Coding Demonstration
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/09/24/Coding-Demonstration/"><span class="article-date">
  2019-09-24
</span>
</a>
        

        

      </div>
      <div class="article-entry">
        
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> d.columns:</span><br><span class="line">    <span class="keyword">if</span> c == <span class="string">"Player"</span> <span class="keyword">or</span> c == <span class="string">"COUNTRY"</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    df = pd.DataFrame(d[c])</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        df.hist(figsize=(<span class="number">7</span>,<span class="number">3</span>),edgecolor=<span class="string">'blue'</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(c,<span class="string">"here"</span>)</span><br><span class="line">plt.title(c)</span><br></pre></td></tr></table></figure>

<p>At this point, we separated all the data into two categories: the physical data(launch angle, driving distance, average swing speed, etc.), and the performance data(stroke gained, sand saves, fairway hits, etc.). We wanted to know whether if there is any physical data that has direct correlation with each player’s points, so we calculated the Pearson correlation of each performance data with the points. In order to do so, we had to normalize the data using normal distribution, since each data varies in the area of being measured and its unit of measurement. </p>
<p><strong>Physical data:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">physical_columns = (<span class="string">"SHORTEST_CARRY_DISTANCE"</span>,<span class="string">"LONGEST_CARRY_DISTANCE"</span>,<span class="string">"AVG_CARRY_DISTANCE"</span>,</span><br><span class="line">                 <span class="string">"SHORTEST_ACT.HANG_TIME"</span>,<span class="string">"SHORTEST_ACT.HANG_TIME"</span>,<span class="string">"LONGEST_ACT.HANG_TIME"</span>,</span><br><span class="line">                <span class="string">"AVG_HANG_TIME"</span>,<span class="string">"LOWEST_SPIN_RATE"</span>,<span class="string">"HIGHEST_SPIN_RATE"</span>,<span class="string">"AVG_SPIN_RATE"</span>,</span><br><span class="line">                <span class="string">"STEEPEST_LAUNCH_ANGLE"</span>,<span class="string">"LOWEST_LAUNCH_ANGLE"</span>,<span class="string">"AVG_LAUNCH_ANGLE"</span>,</span><br><span class="line">                <span class="string">"LOWEST_SF"</span>,<span class="string">"HIGHEST_SF"</span>,<span class="string">"AVG_SMASH_FACTOR"</span>,<span class="string">"SLOWEST_BALL_SPEED"</span>,</span><br><span class="line">                <span class="string">"FASTEST_BALL_SPEED"</span>,<span class="string">"AVG_BALL_SPEED"</span>,<span class="string">"SLOWEST_CH_SPEED"</span>,<span class="string">"FASTEST_CH_SPEED"</span>,</span><br><span class="line">                <span class="string">"AVG_CLUB_HEAD_SPEED"</span>,<span class="string">"DRIVES_320+%"</span>,<span class="string">"RTP-GOING_FOR_THE_GREEN"</span>,<span class="string">"RTP-NOT_GOING_FOR_THE_GRN"</span>,</span><br><span class="line">                <span class="string">"GOING_FOR_GREEN_IN_2%"</span>,<span class="string">"ATTEMPTS_GFG"</span>,<span class="string">"NON-ATTEMPTS_GFG"</span>,<span class="string">"AVG_Driving_DISTANCE"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>Code to normalize data and calculate the Pearson correlation:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.stats</span><br><span class="line">pairs = []</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> physical_columns:</span><br><span class="line">    pearson_co = scipy.stats.pearsonr(points, df[col])[<span class="number">0</span>]</span><br><span class="line">pairs.append([col, pearson_co])</span><br><span class="line"></span><br><span class="line">pairs.sort(key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col, co <span class="keyword">in</span> pairs:</span><br><span class="line">    print(col, co)</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/1.jpg" alt="1.png"></p>
<p>As we can see from all the Pearson Correlation calculated, most of them do not have any strong correlation(-1 as having strong negative correlation and 1 being strong positive correlation). However, one interesting correlation to note is the slightly strong positive correlation in average driving distance, average carry distance, longest carry distance and the number of drives above 320 yards(all of them ranging between 0.40 to 0.47). Furthermore, there is also a slightly strong negative correlation in RTP-going for the green(the total of the score gained (-1 or -2 for example) on the hole relative to par, a player is assumed to be going for the green on the first shot on a par 4 and second shot on a par 5). </p>
<p><strong>Code for scatter plot with drive above 320 and points:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">points = df[<span class="string">"POINTS"</span>]</span><br><span class="line">drive = df[<span class="string">"DRIVES_320+%"</span>]</span><br><span class="line">plt.scatter(points, drive)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/2.jpg" alt="2.png"></p>
<p>this graphs shows the slightly strong positive correlation between points and player’s number of drives above 320. </p>
<p><strong>Code for scatter plot with going for the green relative to par and points:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">points = df[<span class="string">"POINTS"</span>]</span><br><span class="line">RTP = df[<span class="string">"RTP-GOING_FOR_THE_GREEN"</span>]</span><br><span class="line">plt.scatter(points, RTP)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/3.jpg" alt="3.png"></p>
<p>This graph shows the slightly strong negative correlation between points and player’s total going for the green relative to par.</p>
<p>Continuing with the project, we visualized the players’ points distribution using k-means clustering, a type of unsupervised learning that is able to find patterns in the data without pre-existing labels. This method enabled us to partition the data into visible clusters. </p>
<p><strong>Code for k-means clustering (one variable-points):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">1</span>))</span><br><span class="line">d = pd.read_csv(<span class="string">'PGATOUR_meta2.csv'</span>,delimiter = <span class="string">','</span>)</span><br><span class="line">xp = list(d.POINTS)</span><br><span class="line">df= pd.DataFrame(xp)</span><br><span class="line">y = [<span class="number">0</span>] * len(xp)</span><br><span class="line">X = list(zip(y, xp))</span><br><span class="line">k_means = KMeans( n_clusters=<span class="number">5</span>)</span><br><span class="line">k_means.fit(X)</span><br><span class="line">plt.scatter(xp, np.zeros_like(y), c=k_means.labels_, cmap=<span class="string">'rainbow'</span>, s= <span class="number">5</span>)</span><br><span class="line">plt.scatter(k_means.cluster_centers_[:,<span class="number">1</span>] ,k_means.cluster_centers_[:,<span class="number">0</span>],  color=<span class="string">'black'</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">6000</span>, <span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/4.jpg" alt="4.png"></p>
<p>Using similar method, now with several variables, we calculated the k-means clusters centers of each physical data columns with points. This process also required us to normalize the data before calculation. </p>
<p><strong>Code for k-means clustering using scikit learn:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.preprocessing</span><br><span class="line">physical_data = df[list(physical_columns)]</span><br><span class="line">norm_physical_data = physical_data.copy()</span><br><span class="line">scaler = sklearn.preprocessing.StandardScaler()</span><br><span class="line">scaler.fit(norm_physical_data)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">for col in physical_columns:</span></span><br><span class="line"><span class="string">    norm_physical_data[col] = sklearn.preprocessing.norm_physical_data[col]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">norm_physical_data = scaler.transform(norm_physical_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">k_means = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">k_means.fit(norm_physical_data)</span><br><span class="line"></span><br><span class="line">plt.scatter(k_means.labels_, df[<span class="string">"POINTS"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/5.jpg" alt="5.png"></p>
<p><img src="/2019/09/24/Coding-Demonstration/6.jpg" alt="6.png"></p>
<p><img src="/2019/09/24/Coding-Demonstration/7.jpg" alt="7.png"></p>
<p>We plotted this graph with the k-means clusters’ labels and points because it is too difficult and complex to visualize a graph with several variables.</p>
<p>Overall, this project is not successful in predicting players’ ranking in the future, since golf is an extremely complex sports involving many unpredictable variables. Furthermore, we used mostly players’ physical data, which can vary from year to year, and most do not have any direct strong correlation with their points. Interestingly, though, we did find that driving distance and total going for the green relative to par have somewhat strong correlations with players’ points. Base on this observation, we can conclude that driving distances have direct positive relation with the points, and that going for green, measured in negative numbers, has negative relation with the points. In my opinion, I think it is simple to analyze and visualize pre-existing data, but to create a model and make predictions is currently beyond my ability. In the future, I want to improve upon this project and make reasonable predictions using more data and better methods. </p>
<p>Calculating Information gain<br>In machine learning, entropy is the measure of randomness in the data or the information being processed. There are several methods for analyzing and processing data of different types. For two numerical data sets, we can calculate their Pearson correlation, and for two categorical data sets, we can calculate their information gain. Using a generated fake two variable data for example:</p>
<p><strong>Code for calculating entropy:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy_cal</span><span class="params">(probs)</span>:</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> probs:</span><br><span class="line">            res += p*math.log(p)</span><br><span class="line">        <span class="keyword">return</span> -res</span><br></pre></td></tr></table></figure>

<p><strong>Code for calculating information gain:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob_calt</span><span class="params">(al, name1, name2)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">entropy_cal</span><span class="params">(probs)</span>:</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> probs:</span><br><span class="line">            res += p*math.log(p)</span><br><span class="line">        <span class="keyword">return</span> -res</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">series1, series2 = al[name1], al[name2]</span><br><span class="line">    </span><br><span class="line">probs = []</span><br><span class="line">numvars = series1.unique()</span><br><span class="line">series1_list = list(series1)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> numvars:</span><br><span class="line">    e = series1_list.count(var)/len(series1_list)</span><br><span class="line">    probs.append(e)</span><br><span class="line"></span><br><span class="line">entropy1 = entropy_cal(probs)    </span><br><span class="line">    </span><br><span class="line">entropy2 = <span class="number">0</span></span><br><span class="line">numTotal = len(series2)</span><br><span class="line">pe = &#123;k: g[name1].tolist() <span class="keyword">for</span> k,g <span class="keyword">in</span> al.groupby(name2)&#125;</span><br><span class="line"><span class="keyword">for</span> var2 <span class="keyword">in</span> pe:</span><br><span class="line">    prior = len(pe[var2])/numTotal</span><br><span class="line">    numvars1 = set(pe[var2])</span><br><span class="line">    probs = []</span><br><span class="line">    <span class="keyword">for</span> var1 <span class="keyword">in</span> numvars1:</span><br><span class="line">        e = pe[var2].count(var1)/len(pe[var2])</span><br><span class="line">        probs.append(e)</span><br><span class="line"> cond_entropy = entropy_cal(probs)            </span><br><span class="line">        entropy2 += prior * cond_entropy         </span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string">    numvars2 = series2.unique()</span></span><br><span class="line"><span class="string">    series2_list = list(series2)</span></span><br><span class="line"><span class="string">    numTotal = len(series2_list)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    pe = &#123;k: g["0"].tolist() for k,g in al.groupby("COUNTRY")&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    for var in numvars2:</span></span><br><span class="line"><span class="string">        prior = series2_list.count(var)/numTotal</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        # Calculating conditional entropy for var</span></span><br><span class="line"><span class="string">        cond_series1 = series1[series2==var]</span></span><br><span class="line"><span class="string">        numvars1 = cond_series1.unique()</span></span><br><span class="line"><span class="string">        cond_series1_list = list(cond_series1)</span></span><br><span class="line"><span class="string">        probs = []</span></span><br><span class="line"><span class="string">        for var1 in numvars1:</span></span><br><span class="line"><span class="string">            e = cond_series1_list.count(var1)/len(cond_series1_list)</span></span><br><span class="line"><span class="string">            probs.append(e)</span></span><br><span class="line"><span class="string">        cond_entropy = entropy_cal(probs)          </span></span><br><span class="line"><span class="string">    entropy2 += prior * cond_entropy</span></span><br><span class="line"><span class="string">    """</span> </span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    lis2 = input('name of the second col:')</span></span><br><span class="line"><span class="string">    col2_list = al["&#123;&#125;".format(lis2)].tolist()</span></span><br><span class="line"><span class="string">    numvar2 = set(col2_list)</span></span><br><span class="line"><span class="string">    tolnun2 = []</span></span><br><span class="line"><span class="string">    for vars in numvar2:</span></span><br><span class="line"><span class="string">        p = (al[al[lis2].str.contains(vars)])</span></span><br><span class="line"><span class="string">        tolnun2.append(len(p))</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    probs2 = []</span></span><br><span class="line"><span class="string">    for vars in numvar2:</span></span><br><span class="line"><span class="string">        en = col2_list.count(vars)/len(col2_list)</span></span><br><span class="line"><span class="string">        probs2.append(en)</span></span><br><span class="line"><span class="string">        p = (al[al['&#123;&#125;'.format(lis2)].str.contains('vars')])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


        
      </div>
    </div>

  






          <div class="main-footer">
  
    © 2019 Joy&#39;s blog - Powered by <a href="http://hexo.io" target="_blank">Hexo</a> - Theme <a href="https://github.com/denjones/hexo-theme-chan" target="_blank">Chan</a>
  
</div>

      
        </div>
      
    </section></div>
  </div>
  <script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>

  <link rel="stylesheet" href="/PhotoSwipe/photoswipe.css">
  <link rel="stylesheet" href="/PhotoSwipe/default-skin/default-skin.css">

  <!-- Root element of PhotoSwipe. Must have class pswp. -->
  <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
             It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

      <!-- Container that holds slides.
                PhotoSwipe keeps only 3 of them in the DOM to save memory.
                Don't modify these 3 pswp__item elements, data is added later on. -->
      <div class="pswp__container">
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
      </div>

      <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
      <div class="pswp__ui pswp__ui--hidden">

        <div class="pswp__top-bar">

          <!--  Controls are self-explanatory. Order can be changed. -->

          <div class="pswp__counter"></div>

          <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

          <button class="pswp__button pswp__button--share" title="Share"></button>

          <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

          <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

          <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
          <!-- element will get class pswp__preloader--active when preloader is running -->
          <div class="pswp__preloader">
            <div class="pswp__preloader__icn">
              <div class="pswp__preloader__cut">
                <div class="pswp__preloader__donut"></div>
              </div>
            </div>
          </div>
        </div>

        <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
          <div class="pswp__share-tooltip"></div>
        </div>

        <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>

        <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

        <div class="pswp__caption">
          <div class="pswp__caption__center"></div>
        </div>
      </div>
    </div>
  </div>

  <script src="/PhotoSwipe/photoswipe.js"></script>
  <script src="/PhotoSwipe/photoswipe-ui-default.js"></script>


<script src="/perfect-scrollbar/js/min/perfect-scrollbar.min.js"></script>
<script src="/scripts/main.js"></script>

</div></body>
</html>
