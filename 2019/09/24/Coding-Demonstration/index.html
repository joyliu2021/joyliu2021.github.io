<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<title>
  
    Coding Demonstration
  
</title>

<meta name="description" content="123456789for c in d.columns:    if c == &quot;Player&quot; or c == &quot;COUNTRY&quot;:        continue    df = pd.DataFrame(d[c])    try:        df.hist(figsize=(7,3),edgecolor=&apos;blue&apos;)    except:        print(c,&quot;here&quot;)p">
<meta property="og:type" content="article">
<meta property="og:title" content="Coding Demonstration">
<meta property="og:url" content="http://yoursite.com/2019/09/24/Coding-Demonstration/index.html">
<meta property="og:site_name" content="Joy&#39;s blog">
<meta property="og:description" content="123456789for c in d.columns:    if c == &quot;Player&quot; or c == &quot;COUNTRY&quot;:        continue    df = pd.DataFrame(d[c])    try:        df.hist(figsize=(7,3),edgecolor=&apos;blue&apos;)    except:        print(c,&quot;here&quot;)p">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2019/09/24/Coding-Demonstration/1.jpg">
<meta property="og:image" content="http://yoursite.com/2019/09/24/Coding-Demonstration/2.jpg">
<meta property="og:image" content="http://yoursite.com/2019/09/24/Coding-Demonstration/3.jpg">
<meta property="og:image" content="http://yoursite.com/2019/09/24/Coding-Demonstration/4.jpg">
<meta property="og:image" content="http://yoursite.com/2019/09/24/Coding-Demonstration/5.jpg">
<meta property="og:image" content="http://yoursite.com/2019/09/24/Coding-Demonstration/6.jpg">
<meta property="og:image" content="http://yoursite.com/2019/09/24/Coding-Demonstration/7.jpg">
<meta property="og:updated_time" content="2019-09-24T06:52:14.106Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coding Demonstration">
<meta name="twitter:description" content="123456789for c in d.columns:    if c == &quot;Player&quot; or c == &quot;COUNTRY&quot;:        continue    df = pd.DataFrame(d[c])    try:        df.hist(figsize=(7,3),edgecolor=&apos;blue&apos;)    except:        print(c,&quot;here&quot;)p">
<meta name="twitter:image" content="http://yoursite.com/2019/09/24/Coding-Demonstration/1.jpg">


  <link rel="alternative" href="/atom.xml" title="Joy&#39;s blog" type="application/atom+xml">



  <link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="/perfect-scrollbar/css/perfect-scrollbar.min.css">
<link rel="stylesheet" href="/styles/main.css">






</head>
<body class="monochrome">
  <div class="mobile-header">
  <button class="sidebar-toggle" type="button">
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
  </button>
  <a class="title" href="/">Joy&#39;s blog</a>
</div>

  <div class="main-container">
    <div class="sidebar">
  <div class="header">
    <h1 class="title"><a href="/">Joy&#39;s blog</a></h1>
    
    <div class="info">
      <div class="content">
        
        
          <div class="author">Joy</div>
        
      </div>
      
        <div class="avatar">
          
            <a href="/about"><img src="/images/cc/1.jpg"></a>
          
        </div>
      
    </div>
  </div>
  <div class="body">
    
      
        <ul class="nav">
          
            
              <li class="archive-list-container">
                <a href="javascript:;">Archive</a>
                <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">2</span></li></ul>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="/" title="Homepage">Homepage</a>
              </li>
            
          
            
              <li>
                <a href="/archives" title="By Year">By Year</a>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="https://github.com/denjones" title="Github" target="_blank" rel="noopener">Github</a>
              </li>
            
          
        </ul>
      
    
  </div>
</div>

    <div class="main-content">
      
        <div style="max-width: 1000px">
      
          <article id="post-Coding-Demonstration" class="article article-type-post">
  
    <h1 class="article-header">
      Coding Demonstration
    </h1>
  
  

  <div class="article-info">
    <span class="article-date">
  2019-09-24
</span>

    

    

  </div>
  <div class="article-entry">
    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> d.columns:</span><br><span class="line">    <span class="keyword">if</span> c == <span class="string">"Player"</span> <span class="keyword">or</span> c == <span class="string">"COUNTRY"</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    df = pd.DataFrame(d[c])</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        df.hist(figsize=(<span class="number">7</span>,<span class="number">3</span>),edgecolor=<span class="string">'blue'</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(c,<span class="string">"here"</span>)</span><br><span class="line">plt.title(c)</span><br></pre></td></tr></table></figure>

<p>At this point, we separated all the data into two categories: the physical data(launch angle, driving distance, average swing speed, etc.), and the performance data(stroke gained, sand saves, fairway hits, etc.). We wanted to know whether if there is any physical data that has direct correlation with each player’s points, so we calculated the Pearson correlation of each performance data with the points. In order to do so, we had to normalize the data using normal distribution, since each data varies in the area of being measured and its unit of measurement. </p>
<p><strong>Physical data:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">physical_columns = (<span class="string">"SHORTEST_CARRY_DISTANCE"</span>,<span class="string">"LONGEST_CARRY_DISTANCE"</span>,<span class="string">"AVG_CARRY_DISTANCE"</span>,</span><br><span class="line">                 <span class="string">"SHORTEST_ACT.HANG_TIME"</span>,<span class="string">"SHORTEST_ACT.HANG_TIME"</span>,<span class="string">"LONGEST_ACT.HANG_TIME"</span>,</span><br><span class="line">                <span class="string">"AVG_HANG_TIME"</span>,<span class="string">"LOWEST_SPIN_RATE"</span>,<span class="string">"HIGHEST_SPIN_RATE"</span>,<span class="string">"AVG_SPIN_RATE"</span>,</span><br><span class="line">                <span class="string">"STEEPEST_LAUNCH_ANGLE"</span>,<span class="string">"LOWEST_LAUNCH_ANGLE"</span>,<span class="string">"AVG_LAUNCH_ANGLE"</span>,</span><br><span class="line">                <span class="string">"LOWEST_SF"</span>,<span class="string">"HIGHEST_SF"</span>,<span class="string">"AVG_SMASH_FACTOR"</span>,<span class="string">"SLOWEST_BALL_SPEED"</span>,</span><br><span class="line">                <span class="string">"FASTEST_BALL_SPEED"</span>,<span class="string">"AVG_BALL_SPEED"</span>,<span class="string">"SLOWEST_CH_SPEED"</span>,<span class="string">"FASTEST_CH_SPEED"</span>,</span><br><span class="line">                <span class="string">"AVG_CLUB_HEAD_SPEED"</span>,<span class="string">"DRIVES_320+%"</span>,<span class="string">"RTP-GOING_FOR_THE_GREEN"</span>,<span class="string">"RTP-NOT_GOING_FOR_THE_GRN"</span>,</span><br><span class="line">                <span class="string">"GOING_FOR_GREEN_IN_2%"</span>,<span class="string">"ATTEMPTS_GFG"</span>,<span class="string">"NON-ATTEMPTS_GFG"</span>,<span class="string">"AVG_Driving_DISTANCE"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>Code to normalize data and calculate the Pearson correlation:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.stats</span><br><span class="line">pairs = []</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> physical_columns:</span><br><span class="line">    pearson_co = scipy.stats.pearsonr(points, df[col])[<span class="number">0</span>]</span><br><span class="line">pairs.append([col, pearson_co])</span><br><span class="line"></span><br><span class="line">pairs.sort(key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col, co <span class="keyword">in</span> pairs:</span><br><span class="line">    print(col, co)</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/1.jpg" alt="1.png"></p>
<p>As we can see from all the Pearson Correlation calculated, most of them do not have any strong correlation(-1 as having strong negative correlation and 1 being strong positive correlation). However, one interesting correlation to note is the slightly strong positive correlation in average driving distance, average carry distance, longest carry distance and the number of drives above 320 yards(all of them ranging between 0.40 to 0.47). Furthermore, there is also a slightly strong negative correlation in RTP-going for the green(the total of the score gained (-1 or -2 for example) on the hole relative to par, a player is assumed to be going for the green on the first shot on a par 4 and second shot on a par 5). </p>
<p><strong>Code for scatter plot with drive above 320 and points:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">points = df[<span class="string">"POINTS"</span>]</span><br><span class="line">drive = df[<span class="string">"DRIVES_320+%"</span>]</span><br><span class="line">plt.scatter(points, drive)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/2.jpg" alt="2.png"></p>
<p>this graphs shows the slightly strong positive correlation between points and player’s number of drives above 320. </p>
<p><strong>Code for scatter plot with going for the green relative to par and points:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">points = df[<span class="string">"POINTS"</span>]</span><br><span class="line">RTP = df[<span class="string">"RTP-GOING_FOR_THE_GREEN"</span>]</span><br><span class="line">plt.scatter(points, RTP)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/3.jpg" alt="3.png"></p>
<p>This graph shows the slightly strong negative correlation between points and player’s total going for the green relative to par.</p>
<p>Continuing with the project, we visualized the players’ points distribution using k-means clustering, a type of unsupervised learning that is able to find patterns in the data without pre-existing labels. This method enabled us to partition the data into visible clusters. </p>
<p><strong>Code for k-means clustering (one variable-points):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">1</span>))</span><br><span class="line">d = pd.read_csv(<span class="string">'PGATOUR_meta2.csv'</span>,delimiter = <span class="string">','</span>)</span><br><span class="line">xp = list(d.POINTS)</span><br><span class="line">df= pd.DataFrame(xp)</span><br><span class="line">y = [<span class="number">0</span>] * len(xp)</span><br><span class="line">X = list(zip(y, xp))</span><br><span class="line">k_means = KMeans( n_clusters=<span class="number">5</span>)</span><br><span class="line">k_means.fit(X)</span><br><span class="line">plt.scatter(xp, np.zeros_like(y), c=k_means.labels_, cmap=<span class="string">'rainbow'</span>, s= <span class="number">5</span>)</span><br><span class="line">plt.scatter(k_means.cluster_centers_[:,<span class="number">1</span>] ,k_means.cluster_centers_[:,<span class="number">0</span>],  color=<span class="string">'black'</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">6000</span>, <span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/4.jpg" alt="4.png"></p>
<p>Using similar method, now with several variables, we calculated the k-means clusters centers of each physical data columns with points. This process also required us to normalize the data before calculation. </p>
<p><strong>Code for k-means clustering using scikit learn:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.preprocessing</span><br><span class="line">physical_data = df[list(physical_columns)]</span><br><span class="line">norm_physical_data = physical_data.copy()</span><br><span class="line">scaler = sklearn.preprocessing.StandardScaler()</span><br><span class="line">scaler.fit(norm_physical_data)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">for col in physical_columns:</span></span><br><span class="line"><span class="string">    norm_physical_data[col] = sklearn.preprocessing.norm_physical_data[col]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">norm_physical_data = scaler.transform(norm_physical_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">k_means = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">k_means.fit(norm_physical_data)</span><br><span class="line"></span><br><span class="line">plt.scatter(k_means.labels_, df[<span class="string">"POINTS"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2019/09/24/Coding-Demonstration/5.jpg" alt="5.png"></p>
<p><img src="/2019/09/24/Coding-Demonstration/6.jpg" alt="6.png"></p>
<p><img src="/2019/09/24/Coding-Demonstration/7.jpg" alt="7.png"></p>
<p>We plotted this graph with the k-means clusters’ labels and points because it is too difficult and complex to visualize a graph with several variables.</p>
<p>Overall, this project is not successful in predicting players’ ranking in the future, since golf is an extremely complex sports involving many unpredictable variables. Furthermore, we used mostly players’ physical data, which can vary from year to year, and most do not have any direct strong correlation with their points. Interestingly, though, we did find that driving distance and total going for the green relative to par have somewhat strong correlations with players’ points. Base on this observation, we can conclude that driving distances have direct positive relation with the points, and that going for green, measured in negative numbers, has negative relation with the points. In my opinion, I think it is simple to analyze and visualize pre-existing data, but to create a model and make predictions is currently beyond my ability. In the future, I want to improve upon this project and make reasonable predictions using more data and better methods. </p>
<p>Calculating Information gain<br>In machine learning, entropy is the measure of randomness in the data or the information being processed. There are several methods for analyzing and processing data of different types. For two numerical data sets, we can calculate their Pearson correlation, and for two categorical data sets, we can calculate their information gain. Using a generated fake two variable data for example:</p>
<p><strong>Code for calculating entropy:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy_cal</span><span class="params">(probs)</span>:</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> probs:</span><br><span class="line">            res += p*math.log(p)</span><br><span class="line">        <span class="keyword">return</span> -res</span><br></pre></td></tr></table></figure>

<p><strong>Code for calculating information gain:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob_calt</span><span class="params">(al, name1, name2)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">entropy_cal</span><span class="params">(probs)</span>:</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> probs:</span><br><span class="line">            res += p*math.log(p)</span><br><span class="line">        <span class="keyword">return</span> -res</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">series1, series2 = al[name1], al[name2]</span><br><span class="line">    </span><br><span class="line">probs = []</span><br><span class="line">numvars = series1.unique()</span><br><span class="line">series1_list = list(series1)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> numvars:</span><br><span class="line">    e = series1_list.count(var)/len(series1_list)</span><br><span class="line">    probs.append(e)</span><br><span class="line"></span><br><span class="line">entropy1 = entropy_cal(probs)    </span><br><span class="line">    </span><br><span class="line">entropy2 = <span class="number">0</span></span><br><span class="line">numTotal = len(series2)</span><br><span class="line">pe = &#123;k: g[name1].tolist() <span class="keyword">for</span> k,g <span class="keyword">in</span> al.groupby(name2)&#125;</span><br><span class="line"><span class="keyword">for</span> var2 <span class="keyword">in</span> pe:</span><br><span class="line">    prior = len(pe[var2])/numTotal</span><br><span class="line">    numvars1 = set(pe[var2])</span><br><span class="line">    probs = []</span><br><span class="line">    <span class="keyword">for</span> var1 <span class="keyword">in</span> numvars1:</span><br><span class="line">        e = pe[var2].count(var1)/len(pe[var2])</span><br><span class="line">        probs.append(e)</span><br><span class="line"> cond_entropy = entropy_cal(probs)            </span><br><span class="line">        entropy2 += prior * cond_entropy         </span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string">    numvars2 = series2.unique()</span></span><br><span class="line"><span class="string">    series2_list = list(series2)</span></span><br><span class="line"><span class="string">    numTotal = len(series2_list)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    pe = &#123;k: g["0"].tolist() for k,g in al.groupby("COUNTRY")&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    for var in numvars2:</span></span><br><span class="line"><span class="string">        prior = series2_list.count(var)/numTotal</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        # Calculating conditional entropy for var</span></span><br><span class="line"><span class="string">        cond_series1 = series1[series2==var]</span></span><br><span class="line"><span class="string">        numvars1 = cond_series1.unique()</span></span><br><span class="line"><span class="string">        cond_series1_list = list(cond_series1)</span></span><br><span class="line"><span class="string">        probs = []</span></span><br><span class="line"><span class="string">        for var1 in numvars1:</span></span><br><span class="line"><span class="string">            e = cond_series1_list.count(var1)/len(cond_series1_list)</span></span><br><span class="line"><span class="string">            probs.append(e)</span></span><br><span class="line"><span class="string">        cond_entropy = entropy_cal(probs)          </span></span><br><span class="line"><span class="string">    entropy2 += prior * cond_entropy</span></span><br><span class="line"><span class="string">    """</span> </span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    lis2 = input('name of the second col:')</span></span><br><span class="line"><span class="string">    col2_list = al["&#123;&#125;".format(lis2)].tolist()</span></span><br><span class="line"><span class="string">    numvar2 = set(col2_list)</span></span><br><span class="line"><span class="string">    tolnun2 = []</span></span><br><span class="line"><span class="string">    for vars in numvar2:</span></span><br><span class="line"><span class="string">        p = (al[al[lis2].str.contains(vars)])</span></span><br><span class="line"><span class="string">        tolnun2.append(len(p))</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    probs2 = []</span></span><br><span class="line"><span class="string">    for vars in numvar2:</span></span><br><span class="line"><span class="string">        en = col2_list.count(vars)/len(col2_list)</span></span><br><span class="line"><span class="string">        probs2.append(en)</span></span><br><span class="line"><span class="string">        p = (al[al['&#123;&#125;'.format(lis2)].str.contains('vars')])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


  </div>
  <footer class="article-footer">
    
  <div class="cc">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/deed.e" target="_blank" title="Attribution-ShareAlike">
      <img src="/images/cc/cc.png">
      
          <img src="/images/cc/by.png">
        
          <img src="/images/cc/sa.png">
      
      <span>
        This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
      </span>
    </a>
  </div>


    

  </footer>
</article>







          <div class="main-footer">
  
    © 2019 Joy&#39;s blog - Powered by <a href="http://hexo.io" target="_blank">Hexo</a> - Theme <a href="https://github.com/denjones/hexo-theme-chan" target="_blank">Chan</a>
  
</div>

      
        </div>
      
    </div>
  </div>
  <script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>

  <link rel="stylesheet" href="/PhotoSwipe/photoswipe.css">
  <link rel="stylesheet" href="/PhotoSwipe/default-skin/default-skin.css">

  <!-- Root element of PhotoSwipe. Must have class pswp. -->
  <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
             It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

      <!-- Container that holds slides.
                PhotoSwipe keeps only 3 of them in the DOM to save memory.
                Don't modify these 3 pswp__item elements, data is added later on. -->
      <div class="pswp__container">
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
      </div>

      <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
      <div class="pswp__ui pswp__ui--hidden">

        <div class="pswp__top-bar">

          <!--  Controls are self-explanatory. Order can be changed. -->

          <div class="pswp__counter"></div>

          <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

          <button class="pswp__button pswp__button--share" title="Share"></button>

          <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

          <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

          <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
          <!-- element will get class pswp__preloader--active when preloader is running -->
          <div class="pswp__preloader">
            <div class="pswp__preloader__icn">
              <div class="pswp__preloader__cut">
                <div class="pswp__preloader__donut"></div>
              </div>
            </div>
          </div>
        </div>

        <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
          <div class="pswp__share-tooltip"></div>
        </div>

        <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>

        <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

        <div class="pswp__caption">
          <div class="pswp__caption__center"></div>
        </div>
      </div>
    </div>
  </div>

  <script src="/PhotoSwipe/photoswipe.js"></script>
  <script src="/PhotoSwipe/photoswipe-ui-default.js"></script>


<script src="/perfect-scrollbar/js/min/perfect-scrollbar.min.js"></script>
<script src="/scripts/main.js"></script>

</body>
</html>
